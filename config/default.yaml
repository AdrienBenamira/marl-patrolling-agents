agents:
  number_preys: 1
  number_predators: 2

  # For RL
  gamma: 0.9 # Discount factor.
  epsilon_greedy: 0.9 # Probability of exploration.
  lr: 0.01 # Learning rate
  number_actions: 5 # Must be 5 [none, top, left, right, bottom] or 9 with corners added.
  update_frequency: 20 # Update the target net every...
  update_type: hard # Type of update.

replay_memory:
  size: 10000 # Maximum size of the memory.
  shuffle: Yes # If Yes, returns random batches among the elements in the memory. Else always the last ones.

env:
  noise: 0.001 # Agents' actions are not successful with this probability.
  reward_type: full # Must be between full and sparse.
  board_size: 15 # Size of the board
  max_iterations: 50 # Number maximum of steps
  plot_radius: 0.1

  infinite_world: No # If Yes, transported to the other side of the board when crossing border.

learning:
  cuda: Yes # cuda or cpu
  batch_size: 148
  n_episodes: 500000 # Number of episodes
  save_folder: ./builds/
  DDQN: Yes

  plot_episodes_every: 100
  plot_curves_every: 100

  use_model: No # If we load a trained model
  model_path: /path/to/model # path to the trained model

reward:
  coef_distance_reward_predator: 5
  coef_distance_reward_prey: 5
  hot_walls: Yes # If Yes, agents will lose reward if they are against the wall.

  share_wins_among_predators: No # If one predator wins, other predator also get some reward
  reward_if_predators_win: 0.8 # If share_wins_among_predator is Yes, how many reward to give to the other predators (in [-1, 1])
